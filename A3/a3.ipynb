{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "train_path = 'aclImdb/test'\n",
    "test_path = 'aclImdb/train'\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_path, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
    "                train_texts.append(f.read())\n",
    "            if label_type == 'neg':\n",
    "                train_labels.append(0)\n",
    "            else:\n",
    "                train_labels.append(1)\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_path, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
    "                test_texts.append(f.read())\n",
    "            if label_type == 'neg':\n",
    "                test_labels.append(0)\n",
    "            else:\n",
    "                test_labels.append(1)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "vectorizer.fit(test_texts)\n",
    "train_features = vectorizer.transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1402)\t1\n",
      "  (0, 1657)\t1\n",
      "  (0, 2464)\t1\n",
      "  (0, 3258)\t3\n",
      "  (0, 4465)\t2\n",
      "  (0, 6543)\t1\n",
      "  (0, 9881)\t1\n",
      "  (0, 9962)\t2\n",
      "  (0, 13025)\t1\n",
      "  (0, 15056)\t1\n",
      "  (0, 17949)\t1\n",
      "  (0, 21588)\t1\n",
      "  (0, 22050)\t1\n",
      "  (0, 22056)\t1\n",
      "  (0, 23515)\t1\n",
      "  (0, 24536)\t1\n",
      "  (0, 25039)\t1\n",
      "  (0, 25450)\t3\n",
      "  (0, 27312)\t1\n",
      "  (0, 27616)\t1\n",
      "  (0, 28068)\t1\n",
      "  (0, 29490)\t1\n",
      "  (0, 30118)\t1\n",
      "  (0, 31297)\t2\n",
      "  (0, 32517)\t1\n",
      "  :\t:\n",
      "  (24999, 50310)\t1\n",
      "  (24999, 51607)\t1\n",
      "  (24999, 53809)\t1\n",
      "  (24999, 53839)\t1\n",
      "  (24999, 57283)\t1\n",
      "  (24999, 57693)\t1\n",
      "  (24999, 61617)\t1\n",
      "  (24999, 63422)\t1\n",
      "  (24999, 64115)\t1\n",
      "  (24999, 66322)\t1\n",
      "  (24999, 66339)\t4\n",
      "  (24999, 66562)\t4\n",
      "  (24999, 66621)\t1\n",
      "  (24999, 66674)\t1\n",
      "  (24999, 66925)\t2\n",
      "  (24999, 67125)\t3\n",
      "  (24999, 67324)\t1\n",
      "  (24999, 67504)\t1\n",
      "  (24999, 68685)\t1\n",
      "  (24999, 70505)\t1\n",
      "  (24999, 72196)\t2\n",
      "  (24999, 72339)\t1\n",
      "  (24999, 72703)\t1\n",
      "  (24999, 73091)\t1\n",
      "  (24999, 73107)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianNaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        N, D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        # one parameter for each feature conditioned on each class\n",
    "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
    "        Nc = np.zeros(C) # number of instances in class c\n",
    "        # for each class get the MLE for the mean and std\n",
    "        for c in range(C):\n",
    "            x_c = x[y == c]                           #slice all the elements from class c\n",
    "            Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
    "            mu[c,:] = np.mean(x_c,0)                  #mean of features of class c\n",
    "            sigma[c,:] = np.std(x_c, 0)               #std of features of class c\n",
    "            \n",
    "        self.mu = mu                                  # C x D\n",
    "        self.sigma = sigma                            # C x D\n",
    "        self.pi = (Nc+1)/(N+C)                        #Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
